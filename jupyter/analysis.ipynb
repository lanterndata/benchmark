{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d97bea16-bf9a-4b8d-ac8a-eb6ae9b5e246",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The file exists\n"
     ]
    }
   ],
   "source": [
    "# Download initial data\n",
    "\n",
    "import os\n",
    "import urllib.request\n",
    "import gzip\n",
    "import shutil\n",
    "\n",
    "url = 'https://storage.googleapis.com/gresearch/wit/wit_v1.train.all-1percent_sample.tsv.gz'\n",
    "filename = 'data/wit/data.tsv'\n",
    "\n",
    "if os.path.exists(filename):\n",
    "    print(\"The file exists\")\n",
    "\n",
    "else:\n",
    "    # Download the data from the URL\n",
    "    with urllib.request.urlopen(url) as response:\n",
    "      with open(filename + '.gz', 'wb') as f:\n",
    "        f.write(response.read())\n",
    "    \n",
    "    # Extract the data from the compressed file\n",
    "    with gzip.open(filename + '.gz', 'rb') as f_in:\n",
    "      with open(filename, 'wb') as f_out:\n",
    "        shutil.copyfileobj(f_in, f_out)\n",
    "\n",
    "    print(\"The file was downloaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dbc3aa6a-73e9-43fe-90ac-9efac1e748a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create table\n",
      "Copied data\n",
      "Completed\n"
     ]
    }
   ],
   "source": [
    "# Create Postgres table with initial data\n",
    "\n",
    "import os\n",
    "import psycopg2\n",
    "\n",
    "db_connection_string = os.environ.get('DATABASE_URL')\n",
    "conn = psycopg2.connect(db_connection_string)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "with open('data/wit/create_table.sql', 'r') as sql_file:\n",
    "    sql_script = sql_file.read()\n",
    "cursor.execute(sql_script)\n",
    "print(\"Create table\")\n",
    "\n",
    "count_query = \"SELECT COUNT(*) FROM tsv_data\"\n",
    "cursor.execute(count_query)\n",
    "row_count = cursor.fetchone()[0]\n",
    "\n",
    "if row_count == 0:\n",
    "    with open('data/wit/copy_data.sql', 'r') as sql_file:\n",
    "        sql_script = sql_file.read()\n",
    "    cursor.execute(sql_script)\n",
    "    print(\"Copied data\")\n",
    "else:\n",
    "    print(\"No need to copy data\")\n",
    "\n",
    "image_urls_query = \"SELECT id, image_url FROM tsv_data WHERE image_url_ai IS NULL LIMIT 10\"\n",
    "cursor.execute(image_urls_query)\n",
    "image_urls = cursor.fetchall()\n",
    "\n",
    "conn.commit()\n",
    "\n",
    "cursor.close()\n",
    "conn.close()\n",
    "\n",
    "print(\"Completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89300ae9-1a61-408d-9a1d-812f755051ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 441/1000 [04:30<05:35,  1.67it/s]"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "import torch\n",
    "import clip\n",
    "import requests\n",
    "import PIL\n",
    "import io\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "db_connection_string = os.environ.get('DATABASE_URL')\n",
    "conn = psycopg2.connect(db_connection_string)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model, preprocess = clip.load('ViT-B/32', device)\n",
    "model.eval()\n",
    "model.to(device)\n",
    "\n",
    "cursor.execute('''\n",
    "    SELECT\n",
    "        id,\n",
    "        image_url\n",
    "    FROM\n",
    "        tsv_data\n",
    "    WHERE\n",
    "        language = 'en'\n",
    "        AND image_url IS NOT NULL\n",
    "    ORDER BY\n",
    "        RANDOM()\n",
    "    LIMIT 1000\n",
    "''')\n",
    "rows = cursor.fetchall()\n",
    "\n",
    "# Set the batch size for database updates\n",
    "batch_size = 10\n",
    "\n",
    "# Initialize a buffer for batched updates\n",
    "update_buffer = []\n",
    "\n",
    "# Execute batched updates and clear the buffer\n",
    "def update_from_update_buffer():\n",
    "    update_query = \"UPDATE tsv_data SET image_url_ai1 = %s, image_url_ai2 = %s WHERE id = %s\"\n",
    "    cursor.executemany(update_query, update_buffer)\n",
    "    update_buffer.clear()\n",
    "\n",
    "# Process each tuple\n",
    "for item in tqdm(rows):\n",
    "    try:\n",
    "        # Unpack the tuple\n",
    "        id, image_url = item\n",
    "    \n",
    "        # Download the image from the URL\n",
    "        req_headers = {'User-Agent': 'SelectImages/0.0 (narekg.me; ngalstjan4@gmail.com)'}\n",
    "        response = requests.get(image_url, headers=req_headers)\n",
    "        image = PIL.Image.open(io.BytesIO(response.content)).convert(\"RGB\")\n",
    "    \n",
    "        # Preprocess the image and generate embeddings\n",
    "        preprocessed_image = preprocess(image).unsqueeze(0).to(device)\n",
    "        with torch.no_grad():\n",
    "            image_embedding = model.encode_image(preprocessed_image).squeeze().tolist()\n",
    "    \n",
    "        # Add the updated row to the buffer\n",
    "        update_buffer.append((image_embedding, image_embedding, id))\n",
    "    \n",
    "        # Execute batched updates when the buffer reaches the specified batch size\n",
    "        if len(update_buffer) >= batch_size:\n",
    "            update_from_update_buffer()\n",
    "\n",
    "    except Exception as e:\n",
    "        continue\n",
    "\n",
    "# Execute the remaining batched updates in the buffer\n",
    "if len(update_buffer) > 0:\n",
    "    update_from_update_buffer()\n",
    "    \n",
    "# Commit the changes to the database\n",
    "conn.commit()\n",
    "\n",
    "# Close the cursor and database connection\n",
    "cursor.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "90f7e9b6-8f6b-4051-9f31-1f2aaef28630",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [03:28<00:00,  2.40it/s]\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "import torch\n",
    "from transformers import BertModel, BertTokenizer\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import logging\n",
    "logging.getLogger(\"transformers.modeling_utils\").setLevel(logging.ERROR)\n",
    "\n",
    "db_connection_string = os.environ.get('DATABASE_URL')\n",
    "conn = psycopg2.connect(db_connection_string)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load the pre-trained BERT model and tokenizer\n",
    "model_name = 'bert-base-uncased'\n",
    "model = BertModel.from_pretrained(model_name)\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model.eval()\n",
    "model.to(device)\n",
    "\n",
    "cursor.execute('''\n",
    "    SELECT\n",
    "        id,\n",
    "        context_page_description\n",
    "    FROM\n",
    "        tsv_data\n",
    "    WHERE\n",
    "        language = 'en'\n",
    "        AND context_page_description IS NOT NULL\n",
    "    ORDER BY\n",
    "        RANDOM()\n",
    "    LIMIT 500\n",
    "''')\n",
    "rows = cursor.fetchall()\n",
    "\n",
    "# Set the batch size for database updates\n",
    "batch_size = 10\n",
    "\n",
    "# Initialize a buffer for batched updates\n",
    "update_buffer = []\n",
    "\n",
    "# Execute batched updates and clear the buffer\n",
    "def update_from_update_buffer():\n",
    "    update_query = \"UPDATE tsv_data SET context_page_description_ai1 = %s, context_page_description_ai2 = %s WHERE id = %s\"\n",
    "    cursor.executemany(update_query, update_buffer)\n",
    "    update_buffer.clear()\n",
    "\n",
    "# Process each tuple\n",
    "for item in tqdm(rows):\n",
    "    try:\n",
    "        # Unpack the tuple\n",
    "        id, text = item\n",
    "    \n",
    "        # Tokenize the text and generate embeddings\n",
    "        inputs = tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=128,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        inputs = {key: value.to(device) for key, value in inputs.items()}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            text_embedding = torch.mean(outputs.last_hidden_state, dim=1).squeeze().tolist()\n",
    "    \n",
    "        # Add the updated row to the buffer\n",
    "        update_buffer.append((text_embedding, text_embedding, id))\n",
    "    \n",
    "        # Execute batched updates when the buffer reaches the specified batch size\n",
    "        if len(update_buffer) >= batch_size:\n",
    "            update_from_update_buffer()\n",
    "\n",
    "    except Exception as e:\n",
    "        continue\n",
    "\n",
    "# Execute the remaining batched updates in the buffer\n",
    "if len(update_buffer) > 0:\n",
    "    update_from_update_buffer()\n",
    "    \n",
    "# Commit the changes to the database\n",
    "conn.commit()\n",
    "\n",
    "# Close the cursor and database connection\n",
    "cursor.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "da5ce55d-b7a2-4955-8734-3f98e3c08f6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count: 1417\n",
      "select 1: 10493.630222667 OPS\n",
      "select int: 3.302572449754774 OPS\n",
      "pgvector: 3.278139427281616 OPS\n"
     ]
    }
   ],
   "source": [
    "# What is pgvector throughput?\n",
    "import os\n",
    "from pgvector.psycopg2 import register_vector\n",
    "import psycopg2\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "db_connection_string = os.environ.get('DATABASE_URL')\n",
    "conn = psycopg2.connect(db_connection_string)\n",
    "register_vector(conn)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "num_iterations = 10\n",
    "\n",
    "def measure_throughput(query, generate_args=None):\n",
    "    start_time = time.time()\n",
    "\n",
    "    for i in range(num_iterations):\n",
    "        if generate_args is not None:\n",
    "            vector = generate_args()\n",
    "            cursor.execute(query, (vector,))\n",
    "        else:\n",
    "            cursor.execute(query)        \n",
    "    \n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    return num_iterations / elapsed_time\n",
    "\n",
    "cursor.execute(\"select count(*) from tsv_data where image_url_ai IS NOT NULL\")\n",
    "count = cursor.fetchone()[0]\n",
    "\n",
    "print(\"count:\", count)\n",
    "print(f\"select 1: {measure_throughput('SELECT 1')} OPS\")\n",
    "print(f\"select id <: {measure_throughput('SELECT 1 FROM tsv_data WHERE image_url_ai IS NOT NULL AND id < 100' )} OPS\")\n",
    "print(f\"select int <: {measure_throughput('SELECT 1 FROM tsv_data WHERE image_url_ai IS NOT NULL AND original_height < 100' )} OPS\")\n",
    "print(f\"pgvector: {measure_throughput('SELECT * FROM tsv_data WHERE image_url_ai IS NOT NULL AND image_url_ai <-> vector(%s) < 0.5', lambda: np.random.rand(512))} OPS\")\n",
    "\n",
    "cursor.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b825267-c4d2-488a-a3c9-76156e0211a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
